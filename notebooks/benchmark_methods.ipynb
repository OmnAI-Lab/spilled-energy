{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spilled Energy Benchmarking\n",
        "\n",
        "This notebook benchmarks different energy-based hallucination detection methods on the **TriviaQA** dataset.\n",
        "\n",
        "## Goal\n",
        "Evaluate the effectiveness of **Spilled Energy**, **Energy (E)**, and **Marginalized Energy (E_margin)** with different aggregation strategies (**Mean**, **Max**, **Min**, **Sum**) in distinguishing correct answers from hallucinations.\n",
        "\n",
        "## Methodology\n",
        "1.  **Generate** answers using `Meta-Llama-3-8B`.\n",
        "2.  **Extract** exact answers from the generations.\n",
        "3.  **Compute** energy metrics specifically on the **exact answer tokens**.\n",
        "4.  **Split** data into a small Validation set (to find optimal thresholds) and a Test set.\n",
        "5.  **Evaluate** performance using **AUROC** on the Test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "from tqdm.notebook import tqdm\n",
        "import transformers\n",
        "import datasets\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
        "\n",
        "# Add src to path if needed (though we installed in editable mode usually)\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
        "\n",
        "from spilled_energy.generation import generate_answer\n",
        "from spilled_energy.extraction import extract_exact_answer\n",
        "from spilled_energy.energy import spilled_energy\n",
        "\n",
        "# Clean Logging\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "transformers.logging.set_verbosity_error()\n",
        "datasets.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Model & Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: meta-llama/Meta-Llama-3-8B...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ab2c98475864986adde2642ced67a8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TriviaQA...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7852cc43415a454dac8fad25ac6f4bdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29a79f8f941b40e28ae6e445109aa248",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=torch.bfloat16).to(device)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Fallback for testing environments without big GPUs\n",
        "    # MODEL_NAME = \"facebook/opt-125m\"\n",
        "    # ...\n",
        "\n",
        "print(\"Loading TriviaQA...\")\n",
        "dataset = load_dataset(\"trivia_qa\", \"rc\", split=\"validation\", streaming=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_token_metrics(logits, generated_ids, token_start, token_end):\n",
        "    \"\"\"\n",
        "    Computes Spilled, E, E_margin statistics on a specific token slice.\n",
        "    \"\"\"\n",
        "    # Slice\n",
        "    sliced_logits = logits[:, token_start:token_end, :]\n",
        "    sliced_ids = generated_ids[:, token_start:token_end]\n",
        "    \n",
        "    logits_list = sliced_logits.cpu().float().numpy().tolist()\n",
        "    ids_list = sliced_ids.cpu().numpy().tolist()\n",
        "\n",
        "    # Compute raw values\n",
        "    spilled, E_margin, E = spilled_energy(\n",
        "        logits=logits_list,\n",
        "        ids=ids_list,\n",
        "        beta=1.0\n",
        "    )\n",
        "    \n",
        "    # Helper for stats\n",
        "    def get_stats(vals):\n",
        "        v = np.array(vals)\n",
        "        if len(v) == 0: \n",
        "            return {k: 0.0 for k in ['mean', 'max', 'min', 'sum']}\n",
        "        return {\n",
        "            'mean': float(np.mean(v)),\n",
        "            'max': float(np.max(v)),\n",
        "            'min': float(np.min(v)),\n",
        "            'sum': float(np.sum(v))\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'spilled': get_stats(spilled[0]),\n",
        "        'E': get_stats(E[0]),\n",
        "        'E_margin': get_stats(E_margin[0])\n",
        "    }\n",
        "\n",
        "def find_best_threshold(scores, labels):\n",
        "    \"\"\"\n",
        "    Finds threshold that maximizes F1 score.\n",
        "    labels: boolean, True = Hallucination (Positive class)\n",
        "    \"\"\"\n",
        "    if len(set(labels)) < 2:\n",
        "        return 0.0, 0.0\n",
        "        \n",
        "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
        "    f1_scores = 2 * recall * precision / (recall + precision + 1e-10)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else thresholds[-1]\n",
        "    best_f1 = f1_scores[best_idx]\n",
        "    \n",
        "    return best_threshold, best_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Collection Loop\n",
        "We process a number of samples to build our dataset for benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 75 samples...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c15f8344e201432cbcd5922cf10efa1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/75 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "N_VAL = 25   # Validation set size\n",
        "N_TEST = 50  # Test set size\n",
        "TOTAL_SAMPLES = N_VAL + N_TEST\n",
        "\n",
        "results = [] \n",
        "iterator = iter(dataset)\n",
        "\n",
        "print(f\"Processing {TOTAL_SAMPLES} samples...\")\n",
        "\n",
        "for i in tqdm(range(TOTAL_SAMPLES)):\n",
        "    try:\n",
        "        sample = next(iterator)\n",
        "    except StopIteration:\n",
        "        break\n",
        "        \n",
        "    question = sample[\"question\"]\n",
        "    ground_truth_aliases = sample[\"answer\"][\"aliases\"]\n",
        "    \n",
        "    # 1. Generate\n",
        "    prompt = f\"Q: {question}\\nA:\"\n",
        "    gen_output = generate_answer(\n",
        "        prompt=prompt,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        device=\"cuda\"\n",
        "    )\n",
        "    generated_text = gen_output['text']\n",
        "    \n",
        "    # 2. Extract\n",
        "    exact_answer = extract_exact_answer(\n",
        "        question=question,\n",
        "        long_answer=generated_text,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=\"cuda\"\n",
        "    )\n",
        "    cleaned_exact = exact_answer.strip(\"'\\\"\").strip()\n",
        "    \n",
        "    # 3. Check Correctness\n",
        "    is_correct = any(alias.lower() in cleaned_exact.lower() for alias in ground_truth_aliases)\n",
        "    \n",
        "    # 4. Locate Tokens\n",
        "    start_idx = generated_text.find(cleaned_exact)\n",
        "    token_start, token_end = None, None\n",
        "    \n",
        "    if start_idx != -1:\n",
        "        end_idx = start_idx + len(cleaned_exact)\n",
        "        enc = tokenizer(generated_text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "        for t_i, (s, e) in enumerate(enc.offset_mapping):\n",
        "            if s >= start_idx and token_start is None: \n",
        "              token_start = t_i\n",
        "            if s < end_idx: \n",
        "              token_end = t_i + 1\n",
        "    \n",
        "    # 5. Compute Metrics\n",
        "    logits = torch.stack(gen_output['scores'], dim=1)\n",
        "    sequences = gen_output['sequences']\n",
        "    input_len = sequences.shape[1] - logits.shape[1]\n",
        "    generated_ids = sequences[:, input_len:]\n",
        "    \n",
        "    # Fallback to full sequence logic if exact mapping fails\n",
        "    if token_start is None or token_end is None:\n",
        "        token_start, token_end = 0, generated_ids.shape[1]\n",
        "    \n",
        "    token_start = max(0, min(token_start, logits.shape[1]-1))\n",
        "    token_end = max(token_start+1, min(token_end, logits.shape[1]))\n",
        "    \n",
        "    metrics = compute_token_metrics(logits, generated_ids, token_start, token_end)\n",
        "    \n",
        "    split_type = \"val\" if i < N_VAL else \"test\"\n",
        "    results.append({\n",
        "        \"split\": split_type,\n",
        "        \"metrics\": metrics,\n",
        "        \"is_correct\": is_correct,\n",
        "        \"exact_answer\": cleaned_exact,\n",
        "        \"ground_truth\": ground_truth_aliases\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analysis & Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collected 25 validation samples and 50 test samples.\n",
            "Val Accuracy: 48.00%\n",
            "Test Accuracy: 46.00%\n",
            "\n",
            "--- Benchmarking Results (AUROC on Test Set) ---\n",
            "Metric     | Strategy   | Val F1     | Test AUROC | F1 Threshold\n",
            "-----------------------------------------------------------------\n",
            "spilled    | mean       | 0.6842     | 0.4444     | 10.33\n",
            "spilled    | max        | 0.7273     | 0.6135     | 20.85\n",
            "spilled    | min        | 0.6842     | 0.3688     | 0.88\n",
            "spilled    | sum        | 0.7222     | 0.6490     | 20.09\n",
            "E          | mean       | 0.6842     | 0.3849     | -12.94\n",
            "E          | max        | 0.6842     | 0.5733     | 0.00\n",
            "E          | min        | 0.6842     | 0.3833     | -22.38\n",
            "E          | sum        | 0.6842     | 0.3913     | -1294.26\n",
            "E_margin   | mean       | 0.6842     | 0.3849     | -28.01\n",
            "E_margin   | max        | 0.6842     | 0.4783     | -26.39\n",
            "E_margin   | min        | 0.7027     | 0.3575     | -29.63\n",
            "E_margin   | sum        | 0.6842     | 0.3414     | -2327.25\n",
            "\n",
            "Best Performing Method:\n",
            "spilled (sum) - AUROC: 0.6490\n"
          ]
        }
      ],
      "source": [
        "val_data = [r for r in results if r[\"split\"] == \"val\"]\n",
        "test_data = [r for r in results if r[\"split\"] == \"test\"]\n",
        "\n",
        "print(f\"\\nCollected {len(val_data)} validation samples and {len(test_data)} test samples.\")\n",
        "print(f\"Val Accuracy: {np.mean([r['is_correct'] for r in val_data]):.2%}\")\n",
        "print(f\"Test Accuracy: {np.mean([r['is_correct'] for r in test_data]):.2%}\")\n",
        "\n",
        "print(\"\\n--- Benchmarking Results (AUROC on Test Set) ---\")\n",
        "print(f\"{'Metric':<10} | {'Strategy':<10} | {'Val F1':<10} | {'Test AUROC':<10} | {'F1 Threshold':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "aggregated_results = []\n",
        "\n",
        "for metric_name in ['spilled', 'E', 'E_margin']:\n",
        "    for strategy in ['mean', 'max', 'min', 'sum']:\n",
        "        # Prepare scores and labels (Positive label = Hallucination/Incorrect)\n",
        "        # Validation\n",
        "        val_scores = np.array([r['metrics'][metric_name][strategy] for r in val_data])\n",
        "        val_labels = np.array([not r['is_correct'] for r in val_data])\n",
        "        \n",
        "        # Test\n",
        "        test_scores = np.array([r['metrics'][metric_name][strategy] for r in test_data])\n",
        "        test_labels = np.array([not r['is_correct'] for r in test_data])\n",
        "        \n",
        "        # Find Threshold on Val\n",
        "        threshold, best_f1 = find_best_threshold(val_scores, val_labels)\n",
        "        \n",
        "        # Compute AUROC on Test\n",
        "        if len(set(test_labels)) > 1:\n",
        "            auroc = roc_auc_score(test_labels, test_scores)\n",
        "        else:\n",
        "            auroc = 0.5 \n",
        "            \n",
        "        print(f\"{metric_name:<10} | {strategy:<10} | {best_f1:.4f}     | {auroc:.4f}     | {threshold:.2f}\")\n",
        "        aggregated_results.append((metric_name, strategy, auroc))\n",
        "\n",
        "if aggregated_results:\n",
        "    best_metric = max(aggregated_results, key=lambda x: x[2])\n",
        "    print(\"\\nBest Performing Method:\")\n",
        "    print(f\"{best_metric[0]} ({best_metric[1]}) - AUROC: {best_metric[2]:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
